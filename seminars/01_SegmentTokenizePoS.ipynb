{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scB5smameDYo"
   },
   "source": [
    "# Настройка\n",
    "Все зависимости перечислены в ячейке ниже. Кроме того, есть ещё дополнительные данные (opencorpora, например). Они тоже скачиваются в первых ячейках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PoyIMujeDYp",
    "outputId": "86cc7e84-be82-4368-c190-feb4ac05e9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "nltk>=3.4.5\n",
    "razdel>=0.4.0\n",
    "rusenttokenize>=0.0.5\n",
    "b-labs-models>=2017.8.22\n",
    "lxml>=4.2.1\n",
    "spacy>=2.1.4\n",
    "pymystem3>=0.2.0\n",
    "rnnmorph>=0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_3ysaxzVeDYt",
    "outputId": "d5c8e645-9e55-46d5-d8a3-51ee2cd7da0d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !pip install --user --upgrade --force-reinstall -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocGvOjsE9GKh"
   },
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FJ2-w6keDYv",
    "outputId": "91e3c20a-3030-4eb0-c184-9aef554b3514"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Dqa3TH-eDYx",
    "outputId": "4819e8d3-5f15-4429-8a72-674dddd5d284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:40:36.480221: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "     |████████████████████████████████| 13.9 MB 1.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.6.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.12->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.4.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.8.3)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.12->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.16)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A7fL9saeDYz"
   },
   "source": [
    "### Дополнительные данные\n",
    "\n",
    "Opencorpora: 31 Мб по сети, 530 Мб в распакованном виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BnW_XqBeDY0",
    "outputId": "3e1ee0e8-2da0-4556-dd46-8ca4e67a6343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-08 10:11:22--  http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2\n",
      "Resolving opencorpora.org (opencorpora.org)... 164.92.197.18\n",
      "Connecting to opencorpora.org (opencorpora.org)|164.92.197.18|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32754997 (31M) [application/x-bzip2]\n",
      "Saving to: ‘annot.opcorpora.xml.bz2’\n",
      "\n",
      "annot.opcorpora.xml 100%[===================>]  31.24M  41.3MB/s    in 0.8s    \n",
      "\n",
      "2023-03-08 10:11:23 (41.3 MB/s) - ‘annot.opcorpora.xml.bz2’ saved [32754997/32754997]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6wX8NtUveDY2"
   },
   "outputs": [],
   "source": [
    "!bzip2 -d annot.opcorpora.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiKkGUnbeDY4"
   },
   "source": [
    "### Тестовые примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UdqD-e7beDY5"
   },
   "outputs": [],
   "source": [
    "example1 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
    "example2 = \"\"\"\n",
    "    An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.\n",
    "    Assumption Hall, the first student dormitory, was opened in 1954,\n",
    "    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.\n",
    "    It was during the tenure of F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.\n",
    "\"\"\"\n",
    "example3 = \"\"\"\n",
    "    А что насчёт русского языка? Хорошо ли сегментируются имена?\n",
    "    Ай да А.С. Пушкин! Ай да сукин сын!\n",
    "    «Как же так?! Захар...» — воскликнут Пронин.\n",
    "    - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n",
    "    И т. д. и т. п. В общем, вся газета.\n",
    "    Православие... более всего подходит на роль такой идеи...\n",
    "    Нефть за $27/барр. не снится.\n",
    "\"\"\"\n",
    "example4 = \"\"\"\n",
    "    Кружка-термос на 0.5л (50/64 см³, 516;...) стоит $3.88\n",
    "\"\"\"\n",
    "example5 = \"\"\"\n",
    "    Good muffins cost $3.88 in New York.  Please buy me two of them. Thanks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cqtzsw4eDY7"
   },
   "source": [
    "# Сегментация предложений\n",
    "Первая задача - разбиение текста на предложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGebcOPseDY8"
   },
   "source": [
    "### Экперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiEkV_MdeDY9"
   },
   "source": [
    "##### NLTK - Natural Language Toolkit\n",
    "Популярная платформа для анализа текстов. Особенно хорошо работает для английского. В основном не содержит ничего из машинного обучения, только старые добрые правила."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42DWKe5FeDY9",
    "outputId": "d5112a64-ff99-45b2-a0d7-d96b6403d977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " 'Now it’s your turn.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWpr39pPeDZA"
   },
   "source": [
    "А вот тут что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8bXtc-6eDZB",
    "outputId": "17f4c6ea-4a85-4825-c6ca-359f2685e9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    An ambitious campus expansion plan was proposed by Fr.',\n",
       " 'Vernon F. Gallagher in 1952.',\n",
       " 'Assumption Hall, the first student dormitory, was opened in 1954,\\n    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.',\n",
       " 'It was during the tenure of F. Henry J. McAnulty that Fr.',\n",
       " \"Gallagher's ambitious plans were put to action.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dpmBe70eDZE"
   },
   "source": [
    "А что насчёт русского языка?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUNCCwnzeDZF",
    "outputId": "7e02989f-cb56-450f-e349-058f25b10673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    А что насчёт русского языка?',\n",
       " 'Хорошо ли сегментируются имена?',\n",
       " 'Ай да А.С.',\n",
       " 'Пушкин!',\n",
       " 'Ай да сукин сын!',\n",
       " '«Как же так?!',\n",
       " 'Захар...» — воскликнут Пронин.',\n",
       " '- \"Так в чем же дело?\"',\n",
       " '- \"Не ра-ду-ют\".',\n",
       " 'И т. д. и т. п. В общем, вся газета.',\n",
       " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
       " 'не снится.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rWZ0bF4eDZH"
   },
   "source": [
    "https://github.com/Mottl/ru_punkt\n",
    "\n",
    "Data for sentence tokenization was taken from 3 sources:\n",
    "\n",
    "  * Articles from Russian Wikipedia (about 1 million sentences)\n",
    "  * Common Russian abbreviations from Russian orthographic dictionary, edited by V. V. Lopatin;\n",
    "  * Generated names initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nwr9BaZ3eDZI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    А что насчёт русского языка?',\n",
       " 'Хорошо ли сегментируются имена?',\n",
       " 'Ай да А.С. Пушкин!',\n",
       " 'Ай да сукин сын!',\n",
       " '«Как же так?!',\n",
       " 'Захар...» — воскликнут Пронин.',\n",
       " '- \"Так в чем же дело?\"',\n",
       " '- \"Не ра-ду-ют\".',\n",
       " 'И т. д. и т. п. В общем, вся газета.',\n",
       " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
       " 'не снится.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example3, language=\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erWAIVigeDZL"
   },
   "source": [
    "https://github.com/natasha/razdel\n",
    "\n",
    "razdel старается разбивать текст на предложения и токены так, как это сделано в 4 датасетах: SynTagRus, OpenCorpora, ГИКРЯ и РНК из репозитория morphoRuEval-2017.\n",
    "\n",
    "В основном это новостные тексты и литература. Правила razdel заточены под них.\n",
    "\n",
    "На текстах другой тематики (социальные сети, научные статьи) библиотека может работать хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iY1qpHh6eDZL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(5, 33, 'А что насчёт русского языка?'),\n",
       " Substring(34, 65, 'Хорошо ли сегментируются имена?'),\n",
       " Substring(70, 88, 'Ай да А.С. Пушкин!'),\n",
       " Substring(89, 105, 'Ай да сукин сын!'),\n",
       " Substring(110, 123, '«Как же так?!'),\n",
       " Substring(124, 154, 'Захар...» — воскликнут Пронин.'),\n",
       " Substring(159, 181, '- \"Так в чем же дело?\"'),\n",
       " Substring(182, 198, '- \"Не ра-ду-ют\".'),\n",
       " Substring(203, 218, 'И т. д. и т. п.'),\n",
       " Substring(219, 239, 'В общем, вся газета.'),\n",
       " Substring(244,\n",
       "           301,\n",
       "           'Православие... более всего подходит на роль такой идеи...'),\n",
       " Substring(306, 335, 'Нефть за $27/барр. не снится.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "list(sentenize(example3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6DDGryLeDZP"
   },
   "source": [
    "https://github.com/deepmipt/ru_sentence_tokenizer\n",
    "    \n",
    "A simple and fast rule-based sentence segmentation. Tested on OpenCorpora and SynTagRus datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rusenttokenize in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install rusenttokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "x6iYyZ1TeDZQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['А что насчёт русского языка?',\n",
       " 'Хорошо ли сегментируются имена?',\n",
       " 'Ай да А.С. Пушкин!',\n",
       " 'Ай да сукин сын!',\n",
       " '«Как же так?!',\n",
       " 'Захар...» — воскликнут Пронин.',\n",
       " '- \"Так в чем же дело?\"',\n",
       " '- \"Не ра-ду-ют\".',\n",
       " 'И т. д. и т. п.',\n",
       " 'В общем, вся газета.',\n",
       " 'Православие... более всего подходит на роль такой идеи...',\n",
       " 'Нефть за $27/барр. не снится.',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "ru_sent_tokenize(example3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwNAtibneDZS"
   },
   "source": [
    "### Бенчмарки\n",
    "Много вариантов... Нужно измерять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pwae8cmleDZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 110304 sentences from annot.opcorpora.xml\n"
     ]
    }
   ],
   "source": [
    "# WARNING: RAM bound task, XML parsing is expensive\n",
    "# Similar to https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "# \\W -> Any non-word character\n",
    "RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\")\n",
    "\n",
    "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
    "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
    "singles = []\n",
    "compounds = []\n",
    "s2 = sentences.pop().strip()\n",
    "singles.append(s2)\n",
    "while sentences:\n",
    "    s1 = sentences.pop().strip()\n",
    "    singles.append(s1)\n",
    "    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.endswith(':') and not s2.startswith('—'):\n",
    "        compounds.append((s1, s2))\n",
    "    s2 = s1\n",
    "        \n",
    "print(f'Read {len(singles)} sentences from {OPENCORPORA_FILE}')\n",
    "        \n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2havDX5seDZV"
   },
   "outputs": [],
   "source": [
    "def check_sent_tokenizer(tokenizer, singles, compounds):\n",
    "    correct_count_in_singles = 0\n",
    "    for sentence in singles:\n",
    "        correct_count_in_singles += len(tokenizer(sentence)) == 1\n",
    "\n",
    "    correct_count_in_compounds = 0\n",
    "    for s1, s2 in compounds:\n",
    "        correct_count_in_compounds += tokenizer(s1 + ' ' + s2) == [s1, s2]\n",
    "\n",
    "    return (correct_count_in_singles / len(singles), correct_count_in_compounds / len(compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kvWnP6bEeDZY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 7.07 ms, total: 10 s\n",
      "Wall time: 10 s\n",
      "sent_tokenizer scores: 94.30%, 86.07%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n",
    "print(f'sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9f9m7nnneDZc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 37 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n",
      "russian sent_tokenizer scores: 96.78%, 88.85%\n"
     ]
    }
   ],
   "source": [
    "russian_sent_tokenize = lambda s : sent_tokenize(s, language=\"russian\")\n",
    "%time singles_score, compounds_score = check_sent_tokenizer(russian_sent_tokenize, singles, compounds)\n",
    "print(f'russian sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MClqfLRveDZh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.07 s, sys: 0 ns, total: 8.07 s\n",
      "Wall time: 8.07 s\n",
      "razdel scores: 99.07%, 95.39%\n"
     ]
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "razdel_sent_tokenize = lambda text : [s.text for s in sentenize(text)]\n",
    "%time singles_score, compounds_score = check_sent_tokenizer(razdel_sent_tokenize, singles, compounds)\n",
    "print(f'razdel scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IuAOcawmeDZo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.45 s, sys: 0 ns, total: 9.45 s\n",
      "Wall time: 9.45 s\n",
      "deepmipt scores: 98.73%, 93.42%\n"
     ]
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "deepmipt_sent_tokenize = ru_sent_tokenize\n",
    "%time singles_score, compounds_score = check_sent_tokenizer(deepmipt_sent_tokenize, singles, compounds)\n",
    "print(f'deepmipt scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tQJPC51eDZs"
   },
   "source": [
    "Аналогичные бенчмарки:\n",
    "- https://github.com/natasha/razdel/blob/master/eval.ipynb\n",
    "- https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zmDYwWleDZt"
   },
   "source": [
    "### Задание 1: \"Кирпич\"\n",
    "Скачайте предложенный текст. Найдите первое предложение, которое отличается в разбиениях, порождённых rusenttokenize и razdel. Верните номер этого предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uKi00K-seDZu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-08 16:33:49--  https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/q5wo34gfbepc7am/htbg.txt [following]\n",
      "--2023-03-08 16:33:50--  https://www.dropbox.com/s/raw/q5wo34gfbepc7am/htbg.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com/cd/0/inline/B30BVqfuJT8uqnzcQlPBCmaoYhQQX2iw6guPW4XT91lG69rnPjco9G1W_pGvHDhHtRpm3qQC8V57suUyxLrqPPqvXMm0rTJbNWZl4lS7F7zq2HrWZ795YCi3j6Z62dv3OZswg6MnE7R34xRqU05P0bsn9J3w1EBaE5jKBErSUtbQjQ/file# [following]\n",
      "--2023-03-08 16:33:50--  https://uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com/cd/0/inline/B30BVqfuJT8uqnzcQlPBCmaoYhQQX2iw6guPW4XT91lG69rnPjco9G1W_pGvHDhHtRpm3qQC8V57suUyxLrqPPqvXMm0rTJbNWZl4lS7F7zq2HrWZ795YCi3j6Z62dv3OZswg6MnE7R34xRqU05P0bsn9J3w1EBaE5jKBErSUtbQjQ/file\n",
      "Resolving uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com (uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com (uc3ba613fd7318c690d1959535cc.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 592726 (579K) [text/plain]\n",
      "Saving to: ‘htbg.txt.1’\n",
      "\n",
      "htbg.txt.1          100%[===================>] 578.83K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-03-08 16:33:50 (5.16 MB/s) - ‘htbg.txt.1’ saved [592726/592726]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "\n",
    "with open(\"htbg.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oLzXHecqeDZy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    }
   ],
   "source": [
    "def get_first_different_sentence(text: str) -> int:\n",
    "    split_razdel = [sent.text for sent in list(sentenize(text))]\n",
    "    split_rusent = ru_sent_tokenize(text)\n",
    "    idx = 0\n",
    "    for a, b in zip(split_razdel, split_rusent):\n",
    "        if a == b: \n",
    "            idx += 1\n",
    "            continue\n",
    "        else: return idx\n",
    "    return -1\n",
    "\n",
    "assert get_first_different_sentence(text) == 329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_first_different_sentence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQq4iPN8eDZ2",
    "tags": []
   },
   "source": [
    "### Задание 2: Lazy baseline\n",
    "Напишите свой sent_tokenize, который будет делить предложения только по точкам, восклицательным и вопросительным знакам. Измерьте для него время работы и метрики на opencorpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sent_tokenize(text):\n",
    "    sents = []\n",
    "    init_ch = 0\n",
    "    i_add = 1\n",
    "    for i, char in enumerate(text):\n",
    "        if char in ['.', '?', '!', '...', '!..', '?..']:\n",
    "            if i < len(text) - 1 and text[i + 1] in ['?', '!', '.']:\n",
    "                i_add += 1\n",
    "            sent = text[init_ch:i+i_add]\n",
    "            sents.append(sent.strip())\n",
    "            init_ch = i + i_add\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " 'Now it’s your turn.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent_tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " 'Now it’s your turn.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "47iRVTmqeDZ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.61 s, sys: 3.38 ms, total: 5.62 s\n",
      "Wall time: 5.62 s\n",
      "your scores: 80.16%, 72.19%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "assert my_sent_tokenize(example1) == sent_tokenize(example1)\n",
    "\n",
    "%time singles_score, compounds_score = check_sent_tokenizer(my_sent_tokenize, singles, compounds)\n",
    "\n",
    "# assert singles_score >= 0.85\n",
    "\n",
    "print(f'your scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvte2OBUeDZ5"
   },
   "source": [
    "# Токенизация\n",
    "\n",
    "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q0BjSKCneDZ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(example5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "30jl5PpZeDZ8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordDetokenizer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD3EUO5WeDZ_"
   },
   "source": [
    "Они умеют выдавать индексы начала и конца каждого токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Q2z_ZybxeDaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 9), (10, 17), (18, 22), (23, 28), (29, 31), (32, 35), (36, 41), (43, 49), (50, 53), (54, 56), (57, 60), (61, 63), (64, 69), (70, 77)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "print(list(wh_tok.span_tokenize(example5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4HjdWCceDaE"
   },
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OyzF35cOeDaG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qQCmuQpGeDaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    ', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', ' ', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "doc = spacy_nlp(example5, disable=[\"parser\"])\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Yg9w4wNSeDaN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Кружка-термос', 'на', '0.5л', '(', '50/64', 'см³', ',', '516', ';', '...', ')', 'стоит', '$', '3.88']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(example4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bVvJnHfKeDaP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(5, 18, 'Кружка-термос'),\n",
       " Substring(19, 21, 'на'),\n",
       " Substring(22, 25, '0.5'),\n",
       " Substring(25, 26, 'л'),\n",
       " Substring(27, 28, '('),\n",
       " Substring(28, 33, '50/64'),\n",
       " Substring(34, 37, 'см³'),\n",
       " Substring(37, 38, ','),\n",
       " Substring(39, 42, '516'),\n",
       " Substring(42, 43, ';'),\n",
       " Substring(43, 46, '...'),\n",
       " Substring(46, 47, ')'),\n",
       " Substring(48, 53, 'стоит'),\n",
       " Substring(54, 55, '$'),\n",
       " Substring(55, 59, '3.88')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "list(tokenize(example4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVCUdgpOeDaR"
   },
   "source": [
    "### Задание 3: Diff\n",
    "Напишите функцию, которая будет выводить разницу между токенизацией razdel'а и nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher, Differ # USE THIS\n",
    "from razdel import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open(\"htbg.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Аркадий', 'и', 'Борис', 'Стругацкие', 'Трудно']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_razdel = [row.text for row in list(tokenize(text))]\n",
    "split_razdel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Аркадий', 'и', 'Борис', 'Стругацкие', 'Трудно']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_nltk = word_tokenize(text)\n",
    "split_nltk[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64278"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_razdel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63418"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818788372384413"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = SequenceMatcher(a=split_razdel, b=split_nltk)\n",
    "seq.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq.get_matching_blocks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  А', '  р', '  к', '  а', '  д', '  и', '  й']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Differ().compare(split_razdel[0], split_nltk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615\n"
     ]
    }
   ],
   "source": [
    "def get_tokenization_differences(text: str) -> int:\n",
    "    split_razdel = [row.text for row in list(tokenize(text))]\n",
    "    split_nltk = word_tokenize(text)\n",
    "    seq = SequenceMatcher(a=split_razdel, b=split_nltk)\n",
    "    return seq.get_matching_blocks()\n",
    "    \n",
    "print(len(get_tokenization_differences(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gRApSXtKeDaS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(get_tokenization_differences(text)) == 615 # 613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IPBbpoVeDaU",
    "tags": []
   },
   "source": [
    "# Стоп-слова и пунктуация\n",
    "\n",
    "Стоп-слова - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "4CwruKoseDaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3PVbyxz-eDaW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "7rksBLbveDaZ"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAbQc-VzeDab"
   },
   "source": [
    "### Задание 4: Стоп-слова from scratch\n",
    "Постройте свой список стоп-слов на основе Opencorpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "yGRerPDseDac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 110304 sentences from annot.opcorpora.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "from razdel import tokenize\n",
    "\n",
    "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
    "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
    "print(f'Read {len(sentences)} sentences from {OPENCORPORA_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['аркадий', 'и', 'борис', 'стругацкие', 'трудно']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row.text for row in list(tokenize(text.lower()))]\n",
    "sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sent_counts = Counter(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [w[0] for w in sent_counts.most_common(n=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', '–', 'и', 'в']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkGQgO59KZ9G",
    "tags": []
   },
   "source": [
    "# Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "IZsLS4NQKW77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['а', 'что', 'насчет', 'русск', 'язык', '?', 'хорош', 'ли', 'сегментир', 'им', '?', 'а', 'да', 'а', '.', 'с', '.', 'пушкин', '!', 'а', 'да', 'сукин', 'сын', '!', '«', 'как', 'же', 'так', '?!', 'захар', '...', '»', '—', 'воскликнут', 'пронин', '.', '-', '\"', 'так', 'в', 'чем', 'же', 'дел', '?', '\"', '-', '\"', 'не', 'ра-ду-ют', '\"', '.', 'и', 'т', '.', 'д', '.', 'и', 'т', '.', 'п', '.', 'в', 'общ', ',', 'вся', 'газет', '.', 'православ', '...', 'бол', 'всег', 'подход', 'на', 'рол', 'так', 'ид', '...', 'нефт', 'за', '$', '27', '/', 'барр', '.', 'не', 'снит', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from razdel import tokenize\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\") \n",
    "print([stemmer.stem(token.text) for token in tokenize(example3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM6ufubEeDae"
   },
   "source": [
    "# Лемматизация и морфологический анализ\n",
    "\n",
    "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n",
    "* Мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n",
    "* Некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: mystem и pymorphy. С pymorphy всё сразу понятно.\n",
    "\n",
    "Но как работать с Mystem:\n",
    "* Можно скачать mystem и запускать из терминала с разными параметрами\n",
    "* pymystem3 - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfork2jueDaf"
   },
   "source": [
    "## Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "O1ZoKAS5eDag"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "7McqUjduMYxR"
   },
   "outputs": [],
   "source": [
    "! chmod +x /root/.local/bin/mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsSzbzVWeDai"
   },
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "\n",
    "    mystem_bin - путь к mystem, если их несколько\n",
    "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
    "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
    "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uK6iAkmWeDai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '    ', 'а', ' ', 'что', ' ', 'насчет', ' ', 'русский', ' ', 'язык', '? ', 'хорошо', ' ', 'ли', ' ', 'сегментироваться', ' ', 'имя', '?', '\\n', '    ', 'ай', ' ', 'да', ' ', 'а', '.', 'с', '. ', 'пушкин', '! ', 'ай', ' ', 'да', ' ', 'сукин', ' ', 'сын', '!', '\\n', '    «', 'как', ' ', 'же', ' ', 'так', '?! ', 'захар', '...', '» — ', 'восклицать', ' ', 'пронин', '.', '\\n', '    - \"', 'так', ' ', 'в', ' ', 'чем', ' ', 'же', ' ', 'дело', '?\" - ', '\"', 'не', ' ', 'ра', '-', 'ду', '-', 'ют', '\"', '.', '\\n', '    ', 'и', ' ', 'т', '.', ' ', 'д', '.', ' ', 'и', ' ', 'т', '.', ' ', 'п', '. ', 'в', ' ', 'общий', ', ', 'весь', ' ', 'газета', '.', '\\n', '    ', 'православие', '...', ' ', 'много', ' ', 'все', ' ', 'подходить', ' ', 'на', ' ', 'роль', ' ', 'такой', ' ', 'идея', '...', '\\n', '    ', 'нефть', ' ', 'за', ' ', '$', '27', '/', 'барра', '.', ' ', 'не', ' ', 'сниться', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZg3McyqeDal"
   },
   "source": [
    "А можно получить грамматическую информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "-F_PNsqneDal"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n'},\n",
       " {'text': '    '},\n",
       " {'analysis': [{'lex': 'а', 'wt': 0.9822148501, 'gr': 'CONJ='}], 'text': 'А'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'что',\n",
       "    'wt': 0.2934446278,\n",
       "    'gr': 'SPRO,ед,сред,неод=(вин|им)'}],\n",
       "  'text': 'что'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'насчет', 'wt': 1, 'gr': 'PR='}], 'text': 'насчёт'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'русский',\n",
       "    'wt': 0.9496245492,\n",
       "    'gr': 'A=(вин,ед,полн,муж,од|род,ед,полн,муж|род,ед,полн,сред)'}],\n",
       "  'text': 'русского'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'язык', 'wt': 0.9963354032, 'gr': 'S,муж,неод=род,ед'}],\n",
       "  'text': 'языка'},\n",
       " {'text': '? '},\n",
       " {'analysis': [{'lex': 'хорошо', 'wt': 0.0008292704217, 'gr': 'ADV=вводн'}],\n",
       "  'text': 'Хорошо'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'ли', 'wt': 0.7719288688, 'gr': 'PART='}],\n",
       "  'text': 'ли'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'сегментироваться',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,несов,нп=непрош,мн,изъяв,3-л'}],\n",
       "  'text': 'сегментируются'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'имя', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|им,мн)'}],\n",
       "  'text': 'имена'},\n",
       " {'text': '?'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    '},\n",
       " {'analysis': [{'lex': 'ай', 'wt': 0.9841479957, 'gr': 'INTJ='}],\n",
       "  'text': 'Ай'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'да', 'wt': 0.3749187405, 'gr': 'PART='}],\n",
       "  'text': 'да'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'а', 'wt': 0.9822148501, 'gr': 'CONJ='}], 'text': 'А'},\n",
       " {'text': '.'},\n",
       " {'analysis': [{'lex': 'с',\n",
       "    'wt': 2.216901809e-05,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'С'},\n",
       " {'text': '. '},\n",
       " {'analysis': [{'lex': 'пушкин',\n",
       "    'wt': 0.9968761998,\n",
       "    'gr': 'S,фам,муж,од=им,ед'}],\n",
       "  'text': 'Пушкин'},\n",
       " {'text': '! '},\n",
       " {'analysis': [{'lex': 'ай', 'wt': 0.9841479957, 'gr': 'INTJ='}],\n",
       "  'text': 'Ай'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'да', 'wt': 0.6249755963, 'gr': 'CONJ='}],\n",
       "  'text': 'да'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'сукин',\n",
       "    'wt': 1,\n",
       "    'gr': 'A,полн,притяж=(вин,ед,муж,неод|им,ед,муж)'}],\n",
       "  'text': 'сукин'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'сын', 'wt': 1, 'gr': 'S,муж,од=им,ед'}],\n",
       "  'text': 'сын'},\n",
       " {'text': '!'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    «'},\n",
       " {'analysis': [{'lex': 'как', 'wt': 0.3756069802, 'gr': 'ADVPRO='}],\n",
       "  'text': 'Как'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'же', 'wt': 0.9351936974, 'gr': 'PART='}],\n",
       "  'text': 'же'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'так', 'wt': 0.9840554802, 'gr': 'ADVPRO='}],\n",
       "  'text': 'так'},\n",
       " {'text': '?! '},\n",
       " {'analysis': [{'lex': 'захар', 'wt': 1, 'gr': 'S,имя,муж,од=им,ед'}],\n",
       "  'text': 'Захар'},\n",
       " {'text': '...'},\n",
       " {'text': '» — '},\n",
       " {'analysis': [{'lex': 'восклицать',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,нп=непрош,мн,изъяв,3-л,сов'}],\n",
       "  'text': 'воскликнут'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'пронин', 'wt': 1, 'gr': 'S,фам,муж,од=им,ед'}],\n",
       "  'text': 'Пронин'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    - \"'},\n",
       " {'analysis': [{'lex': 'так', 'wt': 0.9840554802, 'gr': 'ADVPRO='}],\n",
       "  'text': 'Так'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'чем', 'wt': 0.8023791472, 'gr': 'CONJ='}],\n",
       "  'text': 'чем'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'же', 'wt': 0.9351936974, 'gr': 'PART='}],\n",
       "  'text': 'же'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'дело',\n",
       "    'wt': 0.9999356856,\n",
       "    'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'дело'},\n",
       " {'text': '?\" - '},\n",
       " {'text': '\"'},\n",
       " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'Не'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'ра'},\n",
       " {'text': '-'},\n",
       " {'analysis': [], 'text': 'ду'},\n",
       " {'text': '-'},\n",
       " {'analysis': [{'lex': 'ют',\n",
       "    'wt': 0.5926217878,\n",
       "    'gr': 'S,муж,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'ют'},\n",
       " {'text': '\"'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    '},\n",
       " {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'И'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'т',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'т'},\n",
       " {'text': '.'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'д',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'д'},\n",
       " {'text': '.'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'т',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'т'},\n",
       " {'text': '.'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'п',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'п'},\n",
       " {'text': '. '},\n",
       " {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'В'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'общий',\n",
       "    'wt': 1,\n",
       "    'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}],\n",
       "  'text': 'общем'},\n",
       " {'text': ', '},\n",
       " {'analysis': [{'lex': 'весь', 'wt': 1, 'gr': 'APRO=им,ед,жен'}],\n",
       "  'text': 'вся'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'газета', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'газета'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    '},\n",
       " {'analysis': [{'lex': 'православие',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'Православие'},\n",
       " {'text': '...'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'много', 'wt': 5.317491368e-05, 'gr': 'ADV=срав'}],\n",
       "  'text': 'более'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'все',\n",
       "    'wt': 0.6138339797,\n",
       "    'gr': 'SPRO,ед,сред,неод=род'}],\n",
       "  'text': 'всего'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'подходить',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,нп=непрош,ед,изъяв,3-л,несов'}],\n",
       "  'text': 'подходит'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'на', 'wt': 0.9989522965, 'gr': 'PR='}], 'text': 'на'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'роль',\n",
       "    'wt': 0.9994611082,\n",
       "    'gr': 'S,жен,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'роль'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'такой',\n",
       "    'wt': 1,\n",
       "    'gr': 'APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|вин,ед,муж,неод|им,ед,муж)'}],\n",
       "  'text': 'такой'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'идея',\n",
       "    'wt': 0.9998233538,\n",
       "    'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}],\n",
       "  'text': 'идеи'},\n",
       " {'text': '...'},\n",
       " {'text': '\\n'},\n",
       " {'text': '    '},\n",
       " {'analysis': [{'lex': 'нефть', 'wt': 1, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}],\n",
       "  'text': 'Нефть'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'за', 'wt': 1, 'gr': 'PR='}], 'text': 'за'},\n",
       " {'text': ' '},\n",
       " {'text': '$'},\n",
       " {'text': '27'},\n",
       " {'text': '/'},\n",
       " {'analysis': [{'lex': 'барра',\n",
       "    'wt': 0.991379209,\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,жен,неод=род,мн'}],\n",
       "  'text': 'барр'},\n",
       " {'text': '.'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'сниться',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}],\n",
       "  'text': 'снится'},\n",
       " {'text': '.'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.analyze(example3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJH0dy1YeDan"
   },
   "source": [
    "## Pymorphy\n",
    "\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "     |████████████████████████████████| 55 kB 1.1 MB/s            \n",
      "\u001b[?25hCollecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "     |████████████████████████████████| 8.2 MB 2.6 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=23d1b52562ed6f73b4c890e51e3e44b2cf04e5f7994edd9a5be02be27f78ebc9\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "h5tsjLpjeDao"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "yOudNTMJeDap"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='мою', tag=OpencorporaTag('ADJF,Apro femn,sing,accs'), normal_form='мой', score=0.970588, methods_stack=((DictionaryAnalyzer(), 'мою', 2049, 10),)),\n",
       " Parse(word='мою', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='мыть', score=0.029411, methods_stack=((DictionaryAnalyzer(), 'мою', 2074, 1),))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy2_analyzer.parse(\"мою\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pymorphy2_analyzer.parse(\"мою\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJF'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мою'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmRbpXNTB72C",
    "tags": []
   },
   "source": [
    "### Задание 5: Анализ частей речи\n",
    "\n",
    "Используя pymorphy2, определите топ-10 самых частотных существительных и глаголов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "My2CZHRwCODO"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "with open(\"htbg.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['аркадий', 'и', 'борис', 'стругацкие', 'трудно']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row.text for row in list(tokenize(text.lower()))]\n",
    "sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c099a477a3c442da513249865cd55d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adjectives = []\n",
    "for word in tqdm(sent):\n",
    "    res = pymorphy2_analyzer.parse(word)\n",
    "    for i, w in enumerate(res):\n",
    "        if res[i].tag.POS == 'ADJF':\n",
    "            adjectives.append(res[i].word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['то', 'то', 'его', 'его', 'его']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "adjectives_counts = Counter(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives_most_common = [w[0] for w in adjectives_counts.most_common(n=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['его',\n",
       " 'рэба',\n",
       " 'их',\n",
       " 'её',\n",
       " 'это',\n",
       " 'все',\n",
       " 'всё',\n",
       " 'цупик',\n",
       " 'нем',\n",
       " 'то',\n",
       " 'благородный',\n",
       " 'этого',\n",
       " 'мой',\n",
       " 'всех',\n",
       " 'другой',\n",
       " 'один',\n",
       " 'такой',\n",
       " 'всего',\n",
       " 'этой',\n",
       " 'сам']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives_most_common[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8qJvkxveDas",
    "tags": []
   },
   "source": [
    "## mystem vs. pymorphy\n",
    "\n",
    "1) Mystem работает невероятно медленно под windows на больших текстах.\n",
    "\n",
    "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "GO55wLFkeDas"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
    "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
    "\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GApdUILveDax"
   },
   "source": [
    "## Rnnmorph\n",
    "Обёртка над pymorphy с разрешением омонимии\n",
    "\n",
    "https://github.com/IlyaGusev/rnnmorph\n",
    "\n",
    "https://habr.com/ru/post/339954/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rnnmorph\n",
      "  Downloading rnnmorph-0.4.1.tar.gz (19.7 MB)\n",
      "     |████████████████████████████████| 19.7 MB 146 kB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (0.24.2)\n",
      "Requirement already satisfied: keras>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (2.8.0)\n",
      "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (3.1.0)\n",
      "Requirement already satisfied: pymorphy2>=0.8 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (0.9.1)\n",
      "Collecting russian-tagsets==0.6\n",
      "  Downloading russian-tagsets-0.6.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (4.63.0)\n",
      "Collecting jsonpickle>=0.9.4\n",
      "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.6/dist-packages (from rnnmorph) (3.6.7)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.6/dist-packages (from h5py>=2.7.0->rnnmorph) (1.5.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle>=0.9.4->rnnmorph) (4.8.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->rnnmorph) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->rnnmorph) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->rnnmorph) (8.0.4)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2>=0.8->rnnmorph) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2>=0.8->rnnmorph) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2>=0.8->rnnmorph) (0.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->rnnmorph) (3.1.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm>=4.14.0->rnnmorph) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle>=0.9.4->rnnmorph) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle>=0.9.4->rnnmorph) (3.7.4.3)\n",
      "Building wheels for collected packages: rnnmorph, russian-tagsets\n",
      "  Building wheel for rnnmorph (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rnnmorph: filename=rnnmorph-0.4.1-py3-none-any.whl size=19746379 sha256=d3927e686850c661460f813c539d4a598abe707bceb019b278dd432d11f48e29\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/cc/02/6a434b98af4e1129902b679a3163b5ca2e56b16244378fdc8c\n",
      "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-py3-none-any.whl size=24636 sha256=57c67e37b33f1bf02605db24647dc2d53d55e50968dacf091e415e30ae2440c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/a7/fe/2aca8a961e93604384b1e0c94925023c9d17539873b433e8fb\n",
      "Successfully built rnnmorph russian-tagsets\n",
      "Installing collected packages: russian-tagsets, jsonpickle, rnnmorph\n",
      "Successfully installed jsonpickle-2.2.0 rnnmorph-0.4.1 russian-tagsets-0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install rnnmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "0nUqNO1leDa8"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2.__internal__' has no attribute 'register_clear_session_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4afb17407919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNNMorphPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrazdel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNMorphPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ru\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhomonym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Косил косой косой косой\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rnnmorph/predictor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrussian_tagsets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTMMorphoAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_from_opencorpora_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_gram_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_form\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordFormOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rnnmorph/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymorphy2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrussian_tagsets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;31m# Inject the clear_session function to keras_deps to remove the dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;31m# from TFLite to Keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_clear_session_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2.__internal__' has no attribute 'register_clear_session_function'"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "from razdel import tokenize\n",
    "\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "homonym = \"Косил косой косой косой\"\n",
    "print(predictor.predict([t.text for t in tokenize(homonym)])[1])\n",
    "print(predictor.predict([t.text for t in tokenize(homonym)])[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeFOWPD17PC3"
   },
   "source": [
    "## GramEval-2020\n",
    "\n",
    "Соревнование по определению морфологических характеристик, определению синтаксических зависимостей и лемматизации. Готовых инструментов не получилось, но весь код всех конкурсантов доступен.\n",
    "* https://github.com/dialogue-evaluation/GramEval2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvFlNWZ1eDbA"
   },
   "source": [
    "### Задание 6: Формат\n",
    "\n",
    "Используя стандартные инструменты переведите корпус htbg.txt в формат CoNLL-U.\n",
    "Используйте следующие колонки: \n",
    "    1. Номер предложения в тексте\n",
    "    2. Токен в том виде, в котором он встретился в тексте\n",
    "    3. Лемма токена\n",
    "    4. POS-таг токена\n",
    "    5. Вектор грамматических значений токена\n",
    "    6. Целевая метка (сделайте метку везде OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfew_CvgeDbB"
   },
   "source": [
    "# Regex 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "CDYSLv67eDbB"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPMiIDMEeDbE"
   },
   "source": [
    "#### match\n",
    "ищет по заданному шаблону в начале строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "OJ4RY9dveDbF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 4), match='abcd'>\n"
     ]
    }
   ],
   "source": [
    "result = re.match('ab+c.', 'abcdefghijkabcabc') # ищем по шаблону 'ab+c.' \n",
    "print (result) # совпадение найдено:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "S2qN9Q1keDbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "print(result.group(0)) # выводим найденное совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Tqg4wISLeDbK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "result = re.match('abc.', 'abdefghijkabcabc')\n",
    "print(result) # совпадение не найдено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dop3xhkteDbN"
   },
   "source": [
    "#### search\n",
    "ищет по всей строке, возвращает только первое найденное совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "PsP6-GN-eDbO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(4, 8), match='abch'>\n"
     ]
    }
   ],
   "source": [
    "result = re.search('ab+c.', 'aefgabchijkabcabc') \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqd_IVNNeDbS"
   },
   "source": [
    "#### findall\n",
    "возвращает список всех найденных совпадений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Ox0yWO57eDbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4ncd6YPeDbV"
   },
   "source": [
    "Вопросы: \n",
    "1) почему нет последнего abc?\n",
    "2) почему нет abcx?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN71xIw6eDbY"
   },
   "source": [
    "#### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "jsWWmhUKeDbZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGR8y8uCeDbc"
   },
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "alEOle-XeDbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDIt0zSKeDbg"
   },
   "source": [
    "#### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "d79CKEy5eDbk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhBHsBTyeDbo"
   },
   "source": [
    "#### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "6r3PuZ3feDbp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile('[А-Яа-яё\\-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_gSGRhKeDbu"
   },
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "0zmDYwWleDZt",
    "KQq4iPN8eDZ2",
    "DAbQc-VzeDab",
    "UmRbpXNTB72C",
    "EvFlNWZ1eDbA",
    "DPMiIDMEeDbE",
    "dop3xhkteDbN",
    "gqd_IVNNeDbS",
    "BN71xIw6eDbY",
    "NDIt0zSKeDbg",
    "PhBHsBTyeDbo"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
